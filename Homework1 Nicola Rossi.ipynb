{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b17c5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import chess\n",
    "import snake\n",
    "\n",
    "###################################################################\n",
    "# Developed by Nicola Rossi (nicolaxrossi) on 19/10/2023\n",
    "# last modified: 20/10/2023\n",
    "###################################################################\n",
    "\n",
    "############################ INTERFACE ############################\n",
    "class GameInterface(metaclass=abc.ABCMeta):\n",
    "    @classmethod\n",
    "    def __subclasshook__(cls, subclass):\n",
    "        return (hasattr(subclass, 'get_initial_configuration') and \n",
    "                callable(subclass.get_initial_configuration) and \n",
    "                hasattr(subclass, 'is_final_config') and \n",
    "                callable(subclass.is_final_config) or \n",
    "                hasattr(subclass, 'get_legal_moves') and \n",
    "                callable(subclass.get_legal_moves) or \n",
    "                \n",
    "                hasattr(subclass, 'apply_move') and \n",
    "                callable(subclass.apply_move) or \n",
    "                hasattr(subclass, 'simulate_move') and \n",
    "                callable(subclass.simulate_move) or \n",
    "                NotImplemented)\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_initial_configuration(self):\n",
    "        \"\"\"return an initial configuration\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def is_final_config(self, configuration):\n",
    "        \"\"\"check whether the configuration is final or not\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_legal_moves(self, configuration):\n",
    "        \"\"\"returns a list containing the set of legal moves, from the given configuration\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def apply_move(self, configuration, move):\n",
    "        \"\"\"applies the moves to the configuration\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def simulate_move(self, configuration, move):\n",
    "        \"\"\" simulates the move on the given configuration: makes a copy of the given config., applies the move on the\n",
    "            copy and returns the copy \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def outcome(self, configuration):\n",
    "        \"\"\"returns information on the outcome\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def update(self, configuration):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def is_failure(self, configuration):\n",
    "        raise NotImplementedError\n",
    "\n",
    "############################ CHESS GAME MODEL ############################\n",
    "class ChessGame(GameInterface):\n",
    "    \n",
    "    def get_initial_configuration(self):\n",
    "        \"\"\"return an initial configuration\"\"\"\n",
    "        return chess.Board()\n",
    "    \n",
    "    def is_final_config(self, configuration:chess.Board):\n",
    "        \"\"\"check whether the configuration is final or not\n",
    "            se configuration.board is None, then the game is not concluded \"\"\"\n",
    "        return configuration.outcome() is not None\n",
    "\n",
    "    def get_legal_moves(self, configuration:chess.Board):\n",
    "        \"\"\"returns a list containing the set of legal moves, from the given configuration\"\"\"\n",
    "        # NOTA: list(configuration.legal_moves) ritorna una lista di mosse del tipo Move.from_uci('g1h3')\n",
    "        return list(configuration.legal_moves)\n",
    "        \n",
    "    def apply_move(self, configuration:chess.Board, move:chess.Move):\n",
    "        \"\"\"applies the moves to the configuration\"\"\"\n",
    "        configuration.push(move)\n",
    "    \n",
    "    def simulate_move(self, configuration:chess.Board, move:chess.Move):\n",
    "        \"\"\"simulates the move on the given configuration: makes a copy of the given config., applies the move on the\n",
    "            copy and returns the copy\"\"\"\n",
    "        copy = configuration.copy()\n",
    "        copy.push(move)\n",
    "        return copy\n",
    "    \n",
    "    def outcome(self, configuration):\n",
    "        \"\"\"returns information on the outcome\"\"\"\n",
    "        outcome_info = {}\n",
    "        outcome_info['Winner'] = configuration.outcome().winner\n",
    "        outcome_info['Termination'] = configuration.outcome().termination\n",
    "        return outcome_info\n",
    "    \n",
    "    def update(self, configuration):\n",
    "        pass\n",
    "    \n",
    "    def is_failure(self, configuration):\n",
    "        pass\n",
    "    \n",
    "\n",
    "############################ SNAKE GAME MODEL ############################\n",
    "class SnakeGame(GameInterface):\n",
    "    \n",
    "    def get_initial_configuration(self):\n",
    "        \"\"\"return an initial configuration\"\"\"\n",
    "        return snake.Board(10, 10)\n",
    "    \n",
    "    def is_final_config(self, configuration:snake.Board):\n",
    "        \"\"\"check whether the configuration is final or not, that is: if the apple has been eaten or not\n",
    "            True -> snake has eaten the apple \"\"\"\n",
    "        return configuration.eaten()\n",
    "\n",
    "    def get_legal_moves(self, configuration:snake.Board):\n",
    "        \"\"\"returns a list containing the set of legal moves, from the given configuration\"\"\"\n",
    "        # NOTE: the 'legal_moves' method returns a list of moves (it's not necessary to convert the call result\n",
    "        # to list type)\n",
    "        return configuration.legal_moves()\n",
    "        \n",
    "    def apply_move(self, configuration:snake.Board, move:str):\n",
    "        \"\"\"applies the moves to the configuration\"\"\"\n",
    "        configuration.apply_move(move)\n",
    "\n",
    "        # check if the snake has eaten an apple. If so, snake must grow in size\n",
    "        if configuration.eaten():\n",
    "            configuration.grow_snake()\n",
    "            #configuration.spawn_apple()\n",
    "    \n",
    "    def simulate_move(self, configuration:snake.Board, move:str):\n",
    "        \"\"\"simulates the move on the given configuration: makes a copy of the given config., applies the move on the\n",
    "            copy and returns the copy\"\"\"\n",
    "        \n",
    "        copy = configuration.copy()\n",
    "        copy.apply_move(move)\n",
    "        \n",
    "        # check if the snake has eaten an apple. If so, snake must grow in size\n",
    "        if copy.eaten():\n",
    "            copy.grow_snake()\n",
    "            #copy.spawn_apple()\n",
    "\n",
    "        return copy\n",
    "    \n",
    "    def update(self, configuration:snake.Board):\n",
    "        if configuration.eaten():\n",
    "            configuration.spawn_apple()\n",
    "            \n",
    "    \n",
    "    def outcome(self, configuration:snake.Board):\n",
    "        \"\"\"returns information on the outcome\"\"\"\n",
    "        outcome_info = {}\n",
    "        outcome_info['Score'] = configuration.score\n",
    "        outcome_info['# Moves'] = configuration.num_moves\n",
    "        outcome_info['Termination'] = configuration.outcome\n",
    "        return outcome_info\n",
    "\n",
    "    def is_failure(self, configuration:snake.Board):\n",
    "        if configuration.snake_collision():\n",
    "            return True\n",
    "        if configuration.wall_collision():\n",
    "            return True\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56a2e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    \"\"\" class defining the concept of state: it contains a representation of the environment, in our case\n",
    "        a possible game configuration (for some game) and a reference to the game, used to compute possible\n",
    "        actions from the state, compute the application of an action to the state and check whether the\n",
    "        state is final or not \"\"\"\n",
    "    \n",
    "    representation = None\n",
    "    game = None\n",
    "    \n",
    "    def __init__(self, g:GameInterface, rep):\n",
    "        self.representation = rep\n",
    "        self.game = g\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, State):\n",
    "            return self.representation == other.representation\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __hash__(self):\n",
    "        # it is required that representation objects have in thei class an implementation of __hash__ method\n",
    "        return hash(self.representation)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"game: {self.game}, representation: \\n{self.representation}\"\n",
    "    \n",
    "    def actions(self):\n",
    "        return self.game.get_legal_moves(self.representation)\n",
    "    \n",
    "    def apply(self, action):\n",
    "        return self.__class__(self.game, self.game.simulate_move(self.representation, action))\n",
    "    \n",
    "    def is_final(self):\n",
    "        return self.game.is_final_config(self.representation)\n",
    "    \n",
    "    def is_failure(self):\n",
    "        return self.game.is_failure(self.representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a6d3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "        \n",
    "        \"\"\" class defined to manage heuristic (and non) search algorithms. It is used because of its convenience\n",
    "            in storing a state and a value for the path-cost function and for a reference to parent node.\n",
    "            In particular, a reference to parent node is very useful to reconstruct (backwards) the sequence\n",
    "            of actions leading from a initial state to a final one (notice action field). \"\"\"\n",
    "        \n",
    "        state = None\n",
    "        action = None\n",
    "        parent = None\n",
    "        path_cost = None\n",
    "        \n",
    "        \"\"\" in a tree, the root node has parent = None \"\"\"\n",
    "        \n",
    "        def __init__(self, s:State, a, p, pc):\n",
    "            self.state = s\n",
    "            self.action = a\n",
    "            self.parent = p\n",
    "            self.path_cost = pc\n",
    "            \n",
    "        def __eq__(self, other):\n",
    "            if isinstance(other, Node):\n",
    "                return self.state == other.state\n",
    "            else:\n",
    "                return False\n",
    "        \n",
    "        def __lt__(self, other):\n",
    "            # no exact notion of order among nodes\n",
    "            return False\n",
    "            \n",
    "        def __hash__(self):\n",
    "            return hash((self.state, self.action, self.parent, self.path_cost))\n",
    "        \n",
    "        def __str__(self):\n",
    "            return f\"SELF: {self.state}, {self.action}, PARENT: {self.parent}, {self.path_cost}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2fc9932",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PathCosts:\n",
    "    \n",
    "    \"\"\" a simple class-collection of path-costs functions (valuating nodes) \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def uniform_cost(n:Node):\n",
    "        \"\"\" if node n' is a successor of node n, then g(n') = g(n) + 1 \"\"\"\n",
    "        return n.path_cost + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20244032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "class Heuristics:\n",
    "    \n",
    "    \"\"\" class-collection of heuristics\n",
    "        Notice that heuristics, in order to assign a value to a state must interface directly with the \n",
    "        module giving methods/representation for the game. So in this class there must be imported\n",
    "        all the game modules one wants to use! \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def snake_h1(s:State):\n",
    "        \n",
    "        # retrieve snake head and apple positions\n",
    "        head_pos = s.representation.snake[0]\n",
    "        apple_pos = s.representation.apple\n",
    "        \n",
    "        # compute Manhattan distance between snake head (P1) and apple (P2), and return it\n",
    "        return abs(apple_pos[0] - head_pos[0]) + abs(apple_pos[1] - head_pos[1])\n",
    "    \n",
    "    @staticmethod\n",
    "    def snake_h2(s:State):\n",
    "        \n",
    "        # retrieve snake head and apple positions\n",
    "        head_pos = s.representation.snake[0]\n",
    "        apple_pos = s.representation.apple\n",
    "        \n",
    "        if s.representation.snake_collision():\n",
    "            return float('inf')\n",
    "        \n",
    "        if s.representation.wall_collision():\n",
    "            return float('inf')\n",
    "        \n",
    "        # compute Manhattan distance between snake head (P1) and apple (P2), and return it\n",
    "        return abs(apple_pos[0] - head_pos[0]) + abs(apple_pos[1] - head_pos[1])\n",
    "        \n",
    "                \n",
    "    @staticmethod\n",
    "    def snake_h3(s:State):\n",
    "        \n",
    "        # IDEA: dato che il serpente cambia direzione random, lasciando alcuni buchi in cui può spawnare la mela\n",
    "        # che portano poi a mangiarsi da solo, si prova a incentivare il rimanere nella stessa direzione, dando una \n",
    "        # penalità quando il serpente cambia direzione\n",
    "        \n",
    "        # retrieve snake head and apple positions\n",
    "        head_pos = s.representation.snake[0]\n",
    "        apple_pos = s.representation.apple\n",
    "        \n",
    "        if s.representation.snake_collision():\n",
    "            return float('inf')\n",
    "        \n",
    "        if s.representation.wall_collision():\n",
    "            return float('inf')\n",
    "        \n",
    "        # if the apple is at distance >= 40% of the maximum distance (about 7) check the snake length\n",
    "        #    if the snake length is 'small', go directly to the apple\n",
    "        #    otherwise, try to make space\n",
    "        # if, instead, the snake is closer to the apple, focus on the apple\n",
    "        # the general idea is: if space is created when the apple is distant, while maybe approaching the apple\n",
    "        # more space will available to manouvre\n",
    "        \n",
    "        print('length: ', len(s.representation.snake))\n",
    "        print('distance: ', abs(apple_pos[0] - head_pos[0]) + abs(apple_pos[1] - head_pos[1]))\n",
    "        \n",
    "        if len(s.representation.snake) <= (0.10 * (s.representation.x_limit * s.representation.y_limit)):\n",
    "            return (abs(apple_pos[0] - head_pos[0]) + abs(apple_pos[1] - head_pos[1]))\n",
    "\n",
    "        else:\n",
    "            \n",
    "            d = (abs(apple_pos[0] - head_pos[0]) + abs(apple_pos[1] - head_pos[1]))\n",
    "\n",
    "            k = 0.2\n",
    "\n",
    "            # Normalize the distance to the range [0, 1] for a 10x10 board\n",
    "            normalized_distance = d / (s.representation.y_limit +  s.representation.x_limit - 2)\n",
    "\n",
    "            # Invert the normalized distance so that shorter distances have higher scores\n",
    "            inverted_normalized_distance = 1 - normalized_distance\n",
    "            \n",
    "            # before adding the random component, first consider if the direction has changed: if so, \n",
    "            # add a malus equal to inverted_normalized_distance to the score\n",
    "\n",
    "            score = inverted_normalized_distance * k\n",
    "            \n",
    "            pred = s.representation.snake[1]\n",
    "            prev_dir = (head_pos[0] - pred[0], head_pos[1] - pred[1])\n",
    "            \n",
    "            if prev_dir != s.representation.snake_direction:\n",
    "                score += (inverted_normalized_distance * k)\n",
    "\n",
    "            # Introduce randomness\n",
    "            mean = score #score*10  # Mean is your calculated score\n",
    "            standard_deviation = 1.0  # Adjust this to control the level of randomness\n",
    "            random_element = abs(random.normalvariate(mean, standard_deviation))\n",
    "\n",
    "        \n",
    "            print('score randomness: ', score + random_element)\n",
    "            # malus se può seguire la sua coda ma non lo fa\n",
    "            tail_pos = s.representation.snake[len(s.representation.snake)-1]\n",
    "            t = (abs(tail_pos[0] - head_pos[0]) + abs(tail_pos[1] - head_pos[1]))\n",
    "            ht_dis = 1 -(t / (s.representation.y_limit +  s.representation.x_limit - 2))\n",
    "            \n",
    "            score += ht_dis\n",
    "\n",
    "            #else:\n",
    "            x_limit = s.representation.x_limit\n",
    "            y_limit = s.representation.y_limit\n",
    "            # incentivare a restare attaccati al muro, quando possibile\n",
    "            #if head_pos[0] == 0 or head_pos[0] == x_limit-1 or head_pos[1] == 0 or head_pos[1] == y_limit-1:\n",
    "            #    return score + random_element\n",
    "            \n",
    "            return score + random_element #+ 2 # + normalized_ht_dis\n",
    "            \n",
    "    \n",
    "    ############################################## SCACCHI ##############################################################\n",
    "    \n",
    "    @staticmethod\n",
    "    def piece_value(piece:chess.Piece):\n",
    "        \n",
    "        if piece.piece_type == chess.PAWN:\n",
    "            return 1\n",
    "        \n",
    "        if piece.piece_type == chess.KNIGHT:\n",
    "            return 3\n",
    "        \n",
    "        if piece.piece_type == chess.BISHOP:\n",
    "            return 3\n",
    "        \n",
    "        if piece.piece_type == chess.ROOK:\n",
    "            return 5\n",
    "        \n",
    "        if piece.piece_type == chess.QUEEN:\n",
    "            return 9\n",
    "        \n",
    "        if piece.piece_type == chess.KING:\n",
    "            return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_king_safety(board):\n",
    "        king_square = board.king(board.turn)\n",
    "\n",
    "        # Evaluate pawn shelter in front of the king\n",
    "        #pawn_shelter_score = Heuristics.evaluate_pawn_shelter(board, king_square)\n",
    "\n",
    "        # Evaluate open files near the king\n",
    "        open_file_score = Heuristics.evaluate_open_files(board, king_square)\n",
    "\n",
    "        # Evaluate the presence of attacking pieces near the king\n",
    "        piece_attack_score = Heuristics.evaluate_piece_attack(board, king_square)\n",
    "\n",
    "        # Evaluate the king's ability to castle\n",
    "        castling_score = Heuristics.evaluate_castling(board)\n",
    "\n",
    "        # Combine the scores based on your preferences\n",
    "        king_safety_score = (open_file_score +\n",
    "                            piece_attack_score + castling_score)\n",
    "    \n",
    "        return king_safety_score\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_pawn_shelter(board, king_square):\n",
    "        # Evaluate the presence of pawns in front of the king\n",
    "        pawn_shelter = 0\n",
    "        for square in chess.SQUARES:\n",
    "            if king_square < square < king_square + 8:\n",
    "                piece = board.piece_at(square)\n",
    "                if piece and piece.color == board.turn and piece.piece_type == chess.PAWN:\n",
    "                    pawn_shelter += 0.1\n",
    "        return pawn_shelter\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_open_files(board, king_square):\n",
    "        # Evaluate the presence of open files near the king\n",
    "        open_file = 0\n",
    "        file = chess.square_file(king_square)\n",
    "        rank = chess.square_rank(king_square)\n",
    "\n",
    "        for offset in [-1, 1]:\n",
    "\n",
    "            adjacent_file = file + offset\n",
    "            adjacent_rank = rank\n",
    "\n",
    "            if 0 <= adjacent_file <= 7:\n",
    "\n",
    "                is_open = True\n",
    "\n",
    "                for i in range(0, 8):\n",
    "\n",
    "                    if adjacent_rank == 7:\n",
    "                        adjacent_rank = 0\n",
    "\n",
    "                    adjacent_square = chess.square(adjacent_file, adjacent_rank)\n",
    "\n",
    "                    if (board.piece_at(adjacent_square) is not None) and (board.piece_at(adjacent_square).color == board.turn):\n",
    "                        is_open = False\n",
    "                        break\n",
    "\n",
    "                    adjacent_rank += 1\n",
    "\n",
    "                if is_open:\n",
    "                    open_file += 0.2\n",
    "\n",
    "        return -open_file\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_piece_attack(board, king_square):\n",
    "        \n",
    "        # Evaluate the presence of attacking pieces near the king\n",
    "        piece_attack = 0\n",
    "\n",
    "        for square in chess.SQUARES:\n",
    "\n",
    "            piece = board.piece_at(square)\n",
    "\n",
    "            if piece and piece.color != board.turn:\n",
    "\n",
    "                if king_square in board.attacks(square):\n",
    "                    piece_attack += 0.3\n",
    "\n",
    "        return -piece_attack\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_castling(board):\n",
    "        # Evaluate the king's ability to castle\n",
    "        if board.turn == chess.WHITE:\n",
    "            if board.has_kingside_castling_rights(board.turn) and board.king(board.turn) == chess.E1:\n",
    "                return 0.2\n",
    "        else:\n",
    "            if board.has_kingside_castling_rights(board.turn) and board.king(board.turn) == chess.E8:\n",
    "                return 0.2\n",
    "\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def chess_h1(s:State):\n",
    "        \"\"\" euristica basata sul material score, per funzionare richiede la libreria 'chess' \"\"\"  \n",
    "        \n",
    "        board = s.representation\n",
    "        \n",
    "        # controllo che lo stato non sia contenga configurazione finale: caso in cui restituisco un valore\n",
    "        # 'grande' per spingere l'algoritmo a scegliere la mossa che termina la partita\n",
    "        if board.is_checkmate():\n",
    "            return 1000000\n",
    "        \n",
    "        white_score = 0\n",
    "        black_score = 0\n",
    "        \n",
    "        for i in range(0, 64):\n",
    "            piece = board.piece_at(chess.parse_square(chess.SQUARE_NAMES[i]))\n",
    "            \n",
    "            # se in uno square non si trova alcuna pedina, viene ritornato None\n",
    "            if piece is not None:\n",
    "                if piece.color == chess.WHITE:\n",
    "                    white_score += Heuristics.piece_value(piece)\n",
    "                else:\n",
    "                    black_score += Heuristics.piece_value(piece)\n",
    "        \n",
    "        score = white_score - black_score\n",
    "        \n",
    "        if board.turn == chess.WHITE:\n",
    "            return score + Heuristics.evaluate_king_safety(board)\n",
    "        else:\n",
    "            return -score + Heuristics.evaluate_king_safety(board)\n",
    "        \n",
    "    @staticmethod\n",
    "    def has_doubled_pawns(board, square):\n",
    "        \"\"\" Check if there are doubled pawns on the same file at the given square. \n",
    "            Notice that it's assumed the piece retrieved is a pawn! \"\"\"\n",
    "\n",
    "        piece = board.piece_at(square)\n",
    "\n",
    "        color = board.turn\n",
    "\n",
    "        file = chess.square_file(square)\n",
    "        rank = chess.square_rank(square)\n",
    "        adjacent_rank = rank\n",
    "\n",
    "        for i in range(0, 8):\n",
    "\n",
    "            if adjacent_rank == 7:\n",
    "                adjacent_rank = 0\n",
    "\n",
    "            if adjacent_rank == rank:\n",
    "                adjacent_rank += 1\n",
    "                continue\n",
    "\n",
    "            adjacent_square = chess.square(file, adjacent_rank)\n",
    "\n",
    "            if (board.piece_at(adjacent_square) is not None) and \\\n",
    "                (board.piece_at(adjacent_square).color == color) and \\\n",
    "                (board.piece_at(adjacent_square).piece_type == chess.PAWN):\n",
    "                return True\n",
    "\n",
    "            adjacent_rank += 1\n",
    "\n",
    "        return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_isolated_pawn(board, square):\n",
    "        \"\"\" Check if a pawn on the given square is isolated (no friendly pawns on ADJACENT files, not the\n",
    "            file where the pawn is actually in).\"\"\"\n",
    "        piece = board.piece_at(square)\n",
    "        if piece is not None and piece.piece_type == chess.PAWN:\n",
    "            file = chess.square_file(square)\n",
    "            rank = chess.square_rank(square)\n",
    "            color = board.turn\n",
    "\n",
    "            # Check the left and right adjacent files for friendly pawns\n",
    "            for offset in [-1, 1]:\n",
    "\n",
    "                adjacent_file = file + offset\n",
    "                print(adjacent_file)\n",
    "                adjacent_rank = rank\n",
    "\n",
    "                if 0 <= adjacent_file <= 7:\n",
    "\n",
    "                    for i in range(0, 8):\n",
    "\n",
    "                        if adjacent_rank == 7:\n",
    "                            adjacent_rank = 0\n",
    "\n",
    "                        adjacent_square = chess.square(adjacent_file, adjacent_rank)\n",
    "\n",
    "                        adjacent_rank += 1\n",
    "\n",
    "                        if (board.piece_at(adjacent_square) is not None) and \\\n",
    "                            (board.piece_at(adjacent_square).color == color) and \\\n",
    "                            (board.piece_at(adjacent_square).piece_type == chess.PAWN):\n",
    "                            return False\n",
    "\n",
    "        # If no adjacent friendly pawns are found, it's an isolated pawn\n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def chess_h2(s:State):   \n",
    "        \"\"\" This function returns a combination of the\n",
    "        \n",
    "           - material score\n",
    "           \n",
    "           - pawn structure: isolated and doubled pawns\n",
    "               \n",
    "           - center control \n",
    "               a little 'reward' is given if the player makes a move to control the center \"\"\"\n",
    "    \n",
    "        board = s.representation\n",
    "        \n",
    "        score = 0\n",
    "\n",
    "        score += Heuristics.chess_h1(s)\n",
    "        \n",
    "        # if score == 1000000 then a final state has been found => return\n",
    "        if score == 1000000:\n",
    "            return score\n",
    "\n",
    "        # Pawn structure (simple evaluation of doubled pawns and isolated pawns)\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece is not None:\n",
    "                if piece == chess.PAWN:\n",
    "                    if Heuristics.is_doubled_pawn(board, square):\n",
    "                        score -= 0.5\n",
    "                    if Heuristics.is_isolated_pawn(board, square):\n",
    "                        score -= 0.5\n",
    "\n",
    "\n",
    "        # Center control\n",
    "        center_squares = [chess.D4, chess.E4, chess.D5, chess.E5]\n",
    "        for square in center_squares:\n",
    "            if board.piece_at(square) is not None:\n",
    "                if board.piece_at(square).color == board.turn:\n",
    "                    score += 0.2\n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7135a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import heapq\n",
    "\n",
    "class Algorithms:\n",
    "    \n",
    "    \"\"\" class containing the implementation of different search algorithms:\n",
    "        - A* search, for single-agent worlds\n",
    "        - minimax (with limited deep) \n",
    "        - alpha-beta pruning minimax \n",
    "        - class Node is implemented to manage A*, deeper information on report \"\"\"\n",
    "    \n",
    "    ################################################### A* ##########################################################\n",
    "    \n",
    "    @staticmethod\n",
    "    def insert(priority_queue, val, count, node:Node):\n",
    "        \"\"\" convenience function to insert elements in the priority queue.\n",
    "            Note: heapq implements min-heaps! \n",
    "            count is used to ensure that equal-valued tuples are mantained in insertion order \"\"\"\n",
    "        heapq.heappush(priority_queue, (val, count, node))\n",
    "    \n",
    "    @staticmethod\n",
    "    def pop(priority_queue):\n",
    "        priority, count, node = heapq.heappop(priority_queue)\n",
    "        return priority, node\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_empty(priority_queue):\n",
    "        return len(priority_queue) == 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_solution(node:Node):\n",
    "        \"\"\" function used to compute the sequence of actions leading to a final state.\n",
    "            In particular, the sequence of actions is reconstructed backwards, starting from the input\n",
    "            node, following the pointer to the father, until the root is found (the root has None parent pointer).\n",
    "            For each visited node, the action is memorized in a list, which is returned.\n",
    "            Notice that in this way actions are returned in the reverse order in which they have to be\n",
    "            applied, so each new action is not appended at the end of the list, but is inserted as the first element.\n",
    "            In this way, the last 'seen' is the first in the list, as it should be. \"\"\"\n",
    "        \n",
    "        action_sequence = []\n",
    "        \n",
    "        action_sequence.insert(0, node.action)\n",
    "        \n",
    "        node = node.parent\n",
    "        \n",
    "        while node.parent is not None:\n",
    "            \n",
    "            action_sequence.insert(0, node.action)\n",
    "            node = node.parent\n",
    "        \n",
    "        return action_sequence\n",
    "            \n",
    "    \n",
    "    @staticmethod\n",
    "    def a_star(s:State, g:PathCosts, h:Heuristics):\n",
    "        \n",
    "        count = 0\n",
    "        \"\"\" This A* implementation, given an initial state, SOLVES the problem. That is, the algorithm returns\n",
    "            a sequence of actions to be applied consecutively, state after state, in order to reach a final state,\n",
    "            that is the goal (ie: solve the problem).\n",
    "            In general , it can happen that a problem does not have a solution/the agent is not able to find one \n",
    "            so the algorithm for such cases returns None, indicating a failure\n",
    "            The algorithm also returns the number of explored states \"\"\"\n",
    "        \n",
    "        # computing h valuation and path cost for the initial state, initializing the first node, the queue and\n",
    "        # the explored set. Inserting the first node to the queue\n",
    "        h_val = h(s)\n",
    "        g_val = 0\n",
    "        f_val = h_val + g_val\n",
    "        \n",
    "        node = Node(s, None, None, g_val)\n",
    "        frontier = []\n",
    "        \n",
    "        Algorithms.insert(frontier, f_val, count, node)\n",
    "        count += 1\n",
    "        \n",
    "        explored = []\n",
    "        \n",
    "        while not Algorithms.is_empty(frontier):\n",
    "            \n",
    "            # recall that the first element of the tuple is the node valuation, the second is the node itself\n",
    "            node = Algorithms.pop(frontier)[1]\n",
    "            \n",
    "            #if node.state.is_failure():\n",
    "            #    continue\n",
    "            \n",
    "            if node.state.is_final():\n",
    "                return Algorithms.compute_solution(node), len(explored)\n",
    "            \n",
    "            explored.append(node)\n",
    "            \n",
    "            for action in node.state.actions():\n",
    "                \n",
    "                # the state obtained applying action to s is computed\n",
    "                new_state = node.state.apply(action)\n",
    "                \n",
    "                # the new state is evaulated under h\n",
    "                h_val = h(new_state)\n",
    "                # the path cost for the new node is evaluated under g\n",
    "                g_val = g(node)\n",
    "                \n",
    "                f_val = h_val + g_val\n",
    "                \n",
    "                child = Node(new_state, action, node, g_val)\n",
    "                \n",
    "                # the new generated child can be already in the frontier, but with a different valuation:\n",
    "                # in order to guarantee optimality, the algorithm must check whether the current child\n",
    "                # is already in the frontier, and if so, if it now has a better valuation.\n",
    "                # If yes, then we need to insert the child with the current valuation and remove the child\n",
    "                # with the previous valuation from the frontier.\n",
    "                # If no, then we check if the child has been already visited, that is: if it is in the 'explored'\n",
    "                # set. If so, we simply don't insert it in the frontier.\n",
    "                \n",
    "                index_to_remove = -1\n",
    "                for i in range(0, len(frontier)):\n",
    "                    # nodes are stored in the second component of the tuple\n",
    "                    if frontier[i][1] == child:\n",
    "\n",
    "                        # nodes valuations are stored in the first component of the tuple\n",
    "                        # notice the - sign, recall that heapq implements only min-heaps, so\n",
    "                        # we need to make the value positive again before comparing it with f_val!\n",
    "                        if frontier[i][0] > f_val:\n",
    "                            index_to_remove = i\n",
    "\n",
    "                # if the child node is already in the frontier, with a worst valuation then the current one (f_val)\n",
    "                # then the tuple (chil, old_val) is removed from the queue and (child, f_val) is inserted\n",
    "                if index_to_remove > -1:\n",
    "                    frontier.pop(index_to_remove)\n",
    "                    Algorithms.insert(frontier, f_val, count, child)\n",
    "                    count += 1\n",
    "                    \n",
    "                \n",
    "                # finally, if child is not explored, it's added to the queue\n",
    "                if child not in explored:\n",
    "                    Algorithms.insert(frontier, f_val, count, child)\n",
    "                    count += 1\n",
    "            \n",
    "        \n",
    "        # if the algorithm has not returned anything until this point, then the whole tree\n",
    "        # has been explored and no solution has been found, a FAILURE must be returned, (None)\n",
    "        return None, len(explored)\n",
    "    \n",
    "    ################################################# GREEDY ########################################################\n",
    "    @staticmethod\n",
    "    def greedy(s:State, g:PathCosts, h:Heuristics):\n",
    "        \n",
    "        count = 0\n",
    "        \"\"\" This Greedy implementation, given an initial state, SOLVES the problem. That is, the algorithm returns\n",
    "            a sequence of actions to be applied consecutively, state after state, in order to reach a final state,\n",
    "            that is the goal (ie: solve the problem).\n",
    "            In general , it can happen that a problem does not have a solution/the agent is not able to find one \n",
    "            so the algorithm for such cases returns None, indicating a failure\n",
    "            The algorithm also returns the number of explored states \"\"\"\n",
    "        \n",
    "        # computing h valuation and path cost for the initial state, initializing the first node, the queue and\n",
    "        # the explored set. Inserting the first node to the queue\n",
    "        h_val = h(s)\n",
    "        \n",
    "        node = Node(s, None, None, 0)\n",
    "        frontier = []\n",
    "        \n",
    "        Algorithms.insert(frontier, h_val, count, node)\n",
    "        count += 1\n",
    "        \n",
    "        explored = []\n",
    "        \n",
    "        while not Algorithms.is_empty(frontier):\n",
    "            \n",
    "            # recall that the first element of the tuple is the node valuation, the second is the node itself\n",
    "            node = Algorithms.pop(frontier)[1]\n",
    "            \n",
    "            explored.append(node)\n",
    "            \n",
    "            for action in node.state.actions():\n",
    "                \n",
    "                # the state obtained applying action to s is computed\n",
    "                new_state = node.state.apply(action)\n",
    "                \n",
    "                # the new state is evaulated under h\n",
    "                h_val = h(new_state)\n",
    "                \n",
    "                child = Node(new_state, action, node, 0)\n",
    "                \n",
    "                # when a state is firstly generated and is final, it's immediately returned\n",
    "                if new_state.is_final():\n",
    "                    return Algorithms.compute_solution(child), len(explored)\n",
    "                    \n",
    "                # if child is not explored, it's added to the queue\n",
    "                if child not in explored:\n",
    "                    Algorithms.insert(frontier, h_val, count, child)\n",
    "                    count += 1\n",
    "            \n",
    "        \n",
    "        # if the algorithm has not returned anything until this point, then the whole tree\n",
    "        # has been explored and no solution has been found, a FAILURE must be returned, (None)\n",
    "        return None, len(explored)\n",
    "        \n",
    "    \n",
    "    ################################################# MINIMAX #######################################################\n",
    "    \n",
    "    @staticmethod\n",
    "    def minimax_search(s:State, l:int, h:Heuristics):\n",
    "        \"\"\" This minimax version is developed to be used in a real-time setting game.\n",
    "            l represents the number of levels to explore during the search \"\"\"\n",
    "        \n",
    "        def min_value(s:State, l:int, h:Heuristics, explored):\n",
    "            \n",
    "            explored += 1\n",
    "        \n",
    "            # base cases\n",
    "            if s.is_final():\n",
    "                return h(s), explored\n",
    "            if l == 0:\n",
    "                return h(s), explored\n",
    "\n",
    "            # other case\n",
    "            v = float('inf')\n",
    "            for action in s.actions():\n",
    "                res, explored = max_value(s.apply(action), l-1, h, explored)\n",
    "                v = min(v, res)\n",
    "                #explored += newly_explored\n",
    "\n",
    "            return v, explored\n",
    "    \n",
    "        def max_value(s:State, l:int, h:Heuristics, explored):\n",
    "            \n",
    "            explored += 1\n",
    "\n",
    "            # base cases\n",
    "            if s.is_final():\n",
    "                return h(s), explored\n",
    "            if l == 0:\n",
    "                return h(s), explored\n",
    "\n",
    "            # other case\n",
    "            v = float('-inf')\n",
    "            for action in s.actions():\n",
    "                res, explored = min_value(s.apply(action), l-1, h, explored)\n",
    "                v = max(v, res)\n",
    "\n",
    "            return v, explored\n",
    "        \n",
    "        valuated_actions = []\n",
    "        \n",
    "        explored = 0\n",
    "        \n",
    "        for action in s.actions():\n",
    "            valuation, explored = min_value(s.apply(action), l, h, explored)\n",
    "            #explored += newly_explored\n",
    "            valuated_actions.append((action, valuation))\n",
    "        \n",
    "        sorted_actions = sorted(valuated_actions, key=lambda x: x[1])\n",
    "        \n",
    "        # if there are two or more actions with the same valuation, the choice is randomized\n",
    "        \n",
    "        # the best action (those with the maximum valuation) is selected\n",
    "        best_valuation = sorted_actions[len(sorted_actions)-1][1]\n",
    "        \n",
    "        to_randomize = []\n",
    "        \n",
    "        # building up the list of action from which to choose\n",
    "        for tup in sorted_actions:\n",
    "            if tup[1] == best_valuation:\n",
    "                to_randomize.append(tup)\n",
    "                \n",
    "        # if there is only one action the maximum valuation, then it's returned\n",
    "        if len(to_randomize) == 1:\n",
    "            return to_randomize[0][0], explored\n",
    "        \n",
    "        # otherwise, the choice is randomized on the list previously created (contains at leat two elements)\n",
    "        else:\n",
    "            return random.choice(to_randomize)[0], explored\n",
    "    \n",
    "    ######################################## MINIMAX ALPHA-BETA PRUNING ############################################\n",
    "    \n",
    "    @staticmethod\n",
    "    def alpha_beta_search(s:State, l:int, h:Heuristics):\n",
    "        \"\"\" This alpha-beta pruning minimax version is developed to be used in a real-time setting game.\n",
    "            l represents the number of levels to explore during the search \"\"\"\n",
    "        \n",
    "        def min_value(s:State, alpha, beta, l:int, h:Heuristics, explored):\n",
    "            \n",
    "            explored += 1\n",
    "            \n",
    "            # base cases\n",
    "            if s.is_final():\n",
    "                return h(s), explored\n",
    "            if l == 0:\n",
    "                return h(s), explored\n",
    "\n",
    "            # other case\n",
    "            v = float('inf')\n",
    "            for action in s.actions():\n",
    "                res, explored = max_value(s.apply(action), alpha, beta, l-1, h, explored)\n",
    "                v = min(v, res)\n",
    "                \n",
    "                if v <= alpha:\n",
    "                    return v, explored\n",
    "                beta = min(beta, v)\n",
    "\n",
    "            return v, explored\n",
    "    \n",
    "        def max_value(s:State, alpha, beta, l:int, h:Heuristics, explored):\n",
    "            \n",
    "            explored += 1\n",
    "\n",
    "            # base cases\n",
    "            if s.is_final():\n",
    "                return h(s), explored\n",
    "            if l == 0:\n",
    "                return h(s), explored\n",
    "\n",
    "            # other case\n",
    "            v = float('-inf')\n",
    "            for action in s.actions():\n",
    "                res, explored = min_value(s.apply(action), alpha, beta, l-1, h, explored)\n",
    "                v = max(v, res)\n",
    "                \n",
    "                if v >= beta:\n",
    "                    return v, explored\n",
    "                alpha = max(alpha, v)\n",
    "\n",
    "            return v, explored\n",
    "        \n",
    "        valuated_actions = []\n",
    "        \n",
    "        explored = 0\n",
    "        \n",
    "        for action in s.actions():\n",
    "            \n",
    "            # initial alpha value = - infinity\n",
    "            # initial beta value = + infinity\n",
    "            valuation, explored = min_value(s.apply(action), float('-inf'), float('inf'), l, h, explored)\n",
    "            valuated_actions.append((action, valuation))\n",
    "        \n",
    "        sorted_actions = sorted(valuated_actions, key=lambda x: x[1])\n",
    "        \n",
    "        # if there are two or more actions with the same valuation, the choice is randomized\n",
    "        \n",
    "        # the best action (those with the maximum valuation) is selected\n",
    "        best_valuation = sorted_actions[len(sorted_actions)-1][1]\n",
    "        \n",
    "        to_randomize = []\n",
    "        \n",
    "        # building up the list of action from which to choose\n",
    "        for tup in sorted_actions:\n",
    "            if tup[1] == best_valuation:\n",
    "                to_randomize.append(tup)\n",
    "                \n",
    "        # if there is only one action the maximum valuation, then it's returned\n",
    "        if len(to_randomize) == 1:\n",
    "            return to_randomize[0][0], explored\n",
    "        \n",
    "        # otherwise, the choice is randomized on the list previously created (contains at leat two elements)\n",
    "        else:\n",
    "            return random.choice(to_randomize)[0], explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0923d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    \"\"\" class defining the concept of agent. An agent is described by:\n",
    "        - a search algorithm the agent has to use (pointer to function defined in Algorithms)\n",
    "        - an heuristic (pointer to function defined in Heuristics) - both for heuristics search and adversarial search\n",
    "        - a path-cost function (pointer to function defined in PathCosts) - only for heuristic search\n",
    "        - number of levels to explore (adversarial search, real-time games) \"\"\"\n",
    "    \n",
    "    algorithm = None\n",
    "    heuristic = None\n",
    "    level = None\n",
    "    path_cost = None\n",
    "    \n",
    "    \n",
    "    def __init__(self, alg:Algorithms, heu:Heuristics, path_cost:PathCosts=None, level=None):\n",
    "        self.algorithm = alg\n",
    "        self.heuristic = heu\n",
    "        \n",
    "        if level is not None:\n",
    "            self.level = level\n",
    "            \n",
    "        if path_cost is not None:\n",
    "            self.path_cost = path_cost\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.algorithm},{self.heuristic},{self.path_cost},{self.level}\"\n",
    "    \n",
    "    def solve(self, s:State):\n",
    "        \"\"\" metodo pensato per giochi a giocatore singolo, in cui gli algoritmi restituiscono direttamente \n",
    "            una soluzione, cioè non è necessario selezionare una mossa alla volta \"\"\"\n",
    "        \n",
    "        if self.path_cost is None:\n",
    "            raise Exception('a path-cost function must be specified!')\n",
    "        \n",
    "        return self.algorithm(s, self.path_cost, self.heuristic)\n",
    "        \n",
    "    def select_action(self, s:State):\n",
    "        \"\"\" metodo pensato per i giochi competitivi, in cui ogni agente sceglie una mossa alla volta, valutando\n",
    "            lo stato in cui si trova l'ambiente al momento della scelta \"\"\"\n",
    "        \n",
    "        if self.level is None:\n",
    "            raise Exception('level must be an integer >= 0!')\n",
    "        \n",
    "        # gli algoritmi minimax e alpha-beta pruning restituiscono una tupla formata da:\n",
    "        # - mossa selezionata\n",
    "        # - valutazione dello stato risultante dall'applicazione di tale mossa\n",
    "        return self.algorithm(s, self.level, self.heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c9980e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "import stats\n",
    "import pandas as pd\n",
    "\n",
    "class Infrastructure:\n",
    "    \n",
    "    agents = []\n",
    "    game = None\n",
    "    \n",
    "    def __init__(self, agents:list[Agent], game:GameInterface, stat:stats.Statistics):\n",
    "        self.agents = agents\n",
    "        self.game = game\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.agents}, {self.game}\"\n",
    "    \n",
    "    def set_game(self, game:GameInterface):\n",
    "        self.game = game\n",
    "        \n",
    "    def set_agents(self, agents:list[Agent]):\n",
    "        self.agents = agents\n",
    "    \n",
    "    # main: environment simulation\n",
    "    def main(self):\n",
    "        \n",
    "        if self.game is None:\n",
    "            raise Exception('game model cannot be None!')\n",
    "        \n",
    "        if len(self.agents) == 0 or self.agents is None:\n",
    "            raise Exception('instantiate with at least one agent!')\n",
    "            \n",
    "        # initial state\n",
    "        s = State(self.game, self.game.get_initial_configuration())\n",
    "        \n",
    "        # single-player environment\n",
    "        if len(self.agents) == 1:\n",
    "            \n",
    "            # varibale used to have an approximation of the cumulative time needed to solve each sub-problem\n",
    "            cumulative_time = 0\n",
    "            \n",
    "            # varibale used to count the number of explored state\n",
    "            cumulative_explored = 0\n",
    "            \n",
    "            failure = False\n",
    "            \n",
    "            while not failure:\n",
    "            \n",
    "                # recall: solve(s) return the sequence of actions needed to solve the problem!\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                explored = 0\n",
    "                \n",
    "                solution, explored = self.agents[0].solve(s)\n",
    "                \n",
    "                end_time = time.time()\n",
    "                \n",
    "                cumulative_explored += explored\n",
    "                \n",
    "                cumulative_time += (end_time - start_time)\n",
    "                \n",
    "                # check whether the algorithm has returned a failure:\n",
    "                if solution is not None:\n",
    "                    for action in solution:\n",
    "                        s = s.apply(action)\n",
    "                        \n",
    "                        time.sleep(0.3)\n",
    "                        print(s.representation)\n",
    "                        \n",
    "                        if s.is_failure():\n",
    "                            failure = True\n",
    "                            print('failure')\n",
    "                            print(self.game.outcome(s.representation))\n",
    "                            outcome = self.game.outcome(s.representation)\n",
    "                            num_moves = outcome['# Moves']\n",
    "                            score = outcome['Score']\n",
    "                            fail_cause = outcome['Termination']\n",
    "                            \n",
    "                            # add new record for statistics\n",
    "                            record = [cumulative_time, cumulative_explored, num_moves, score, fail_cause]\n",
    "                            stat.insert_record(record)\n",
    "                            \n",
    "                            break\n",
    "                        \n",
    "                        clear_output(wait=True)\n",
    "                        \n",
    "                    self.game.update(s.representation)\n",
    "            \n",
    "        # multi-player environment (at least 2 agents)\n",
    "        elif len(self.agents) > 1:\n",
    "            \n",
    "            # testing/debugging\n",
    "            print(s.representation)\n",
    "            \n",
    "            # dictionary collecting information of each player\n",
    "            # each player is associated with:\n",
    "            # [0] -> total game time (sum of times needed to make a move), initialized to 0\n",
    "            # [1] -> number of moves, initialized to 0 (a player's turn basically means a move)\n",
    "            # [2] -> total number of states explored to select the move, initialized to zero\n",
    "            game_stats = {}\n",
    "            \n",
    "            for i in range(0, len(self.agents)):\n",
    "                game_stats[i] = [0, 0, 0]\n",
    "            \n",
    "            while not s.is_final():\n",
    "                for i in range(0, len(self.agents)):\n",
    "                    \n",
    "                    # l'agente i-esimo sceglie la propria azione\n",
    "                    starting_time = time.time()\n",
    "                    \n",
    "                    current_agent_action, explored = self.agents[i].select_action(s)\n",
    "                    \n",
    "                    finishing_time = time.time()\n",
    "                    \n",
    "                    delta_t = finishing_time - starting_time\n",
    "                    game_stats[i][0] += delta_t\n",
    "                    game_stats[i][1] += 1\n",
    "                    game_stats[i][2] += explored\n",
    "                    \n",
    "                    # l'azione scelta dall'agente i-esimo viene applicata allo stato attuale,\n",
    "                    # un nuovo stato viene generato\n",
    "                    s = s.apply(current_agent_action)\n",
    "                        \n",
    "                    time.sleep(0.3)\n",
    "                    print(s.representation)\n",
    "                    \n",
    "                    # controllo necessario per gestire il caso in cui l'agente ai, i < len(agents) esegue\n",
    "                    # un'azione che trasforma lo stato corrente in uno finale.\n",
    "                    if s.is_final():\n",
    "                        print('game ended!')\n",
    "                        # in chess case, we have that the player 0 is white and the chess module\n",
    "                        # returns True if White wins, False otherwise and None if drawn\n",
    "                        result = self.game.outcome(s.representation)\n",
    "                        termination = result['Termination']\n",
    "                        \n",
    "                        if result['Winner'] is not None:\n",
    "                            if result['Winner'] is True:\n",
    "                                winner = 'W'\n",
    "                            else:\n",
    "                                winner = 'B'\n",
    "                        else:\n",
    "                            winner = 'draw'\n",
    "                        \n",
    "                        # returning a dataframe containing collected game statistics and outcome info\n",
    "                        df = pd.DataFrame(columns=['PLAYER', 'TIME', '# MOVES', '# EXPLORED', 'OUTCOME', 'HAS WIN'])\n",
    "                        for i in range(0, len(self.agents)):\n",
    "                            \n",
    "                            l = []\n",
    "                            \n",
    "                            l.append(i)\n",
    "                            \n",
    "                            l.extend(game_stats[i])\n",
    "                            \n",
    "                            l.append(termination)\n",
    "                            \n",
    "                            if winner == 'draw':\n",
    "                                l.append(0)\n",
    "                            \n",
    "                            if winner == 'W':\n",
    "                                if i == 0:\n",
    "                                    l.append(1)\n",
    "                                else:\n",
    "                                    l.append(0)\n",
    "                            \n",
    "                            if winner == 'B':\n",
    "                                if i == 1:\n",
    "                                    l.append(1)\n",
    "                                else:\n",
    "                                    l.append(0)\n",
    "                                    \n",
    "                            df.loc[i] = l\n",
    "                        \n",
    "                        return df\n",
    "                        break\n",
    "                    \n",
    "                    clear_output(wait=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ddc142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90442180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(stat, agents, game, sample_size):\n",
    "\n",
    "    i = Infrastructure(agents, snake_g, stat)\n",
    "    \n",
    "    for j in range(0, sample_size):\n",
    "        print(f'collecting {j+1}-th sample ...')\n",
    "        time.sleep(0.7)\n",
    "        \n",
    "        i.main()\n",
    "        # the information collected during this experiment, is added to stat\n",
    "        # that is, added to stat dataframe and will be used to calculate statistics\n",
    "        print('collected! Starting again ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c1c434f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat = stats.Statistics(['TIME', '# EXPLORED',  '# MOVES', 'SCORE', 'FAILURE CAUSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8823adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = Agent(Algorithms.greedy, Heuristics.snake_h3, path_cost=PathCosts.uniform_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "900d1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "snake_g = SnakeGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5a3a426",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1\n"
     ]
    }
   ],
   "source": [
    "n = int(input('Sample size: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab9e7fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length:  34\n",
      "distance:  12\n",
      "score randomness:  0.7862255261099895\n",
      "length:  34\n",
      "distance:  11\n",
      "score randomness:  1.1286006589295248\n",
      "length:  34\n",
      "distance:  14\n",
      "score randomness:  0.18892323377280507\n",
      "length:  34\n",
      "distance:  15\n",
      "score randomness:  1.552813408796752\n",
      "length:  34\n",
      "distance:  13\n",
      "score randomness:  1.5736232819676963\n",
      "length:  34\n",
      "distance:  12\n",
      "score randomness:  0.6842663986885729\n",
      "length:  34\n",
      "distance:  11\n",
      "score randomness:  0.3087958911064191\n",
      "length:  34\n",
      "distance:  10\n",
      "score randomness:  0.4382527365558985\n",
      "length:  34\n",
      "distance:  10\n",
      "score randomness:  2.5682089625233875\n",
      "length:  34\n",
      "distance:  9\n",
      "score randomness:  0.9151076713846845\n",
      "length:  34\n",
      "distance:  10\n",
      "score randomness:  1.2520950642199744\n",
      "length:  34\n",
      "distance:  8\n",
      "score randomness:  1.2162929533219462\n",
      "length:  34\n",
      "distance:  8\n",
      "score randomness:  1.2910191805329416\n",
      "length:  34\n",
      "distance:  9\n",
      "score randomness:  0.4867168547969287\n",
      "length:  34\n",
      "distance:  8\n",
      "score randomness:  0.4703548120305184\n",
      "length:  34\n",
      "distance:  8\n",
      "score randomness:  1.4313240180763789\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  0.515060984026928\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  0.944361321086414\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  0.1380944466944173\n",
      "length:  34\n",
      "distance:  8\n",
      "score randomness:  1.189738238431744\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  1.0080763286152434\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  0.8387976094439992\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  0.6964505147085865\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  0.7888173285615794\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  0.7035048813211724\n",
      "length:  34\n",
      "distance:  4\n",
      "score randomness:  0.22449089677493622\n",
      "length:  34\n",
      "distance:  4\n",
      "score randomness:  0.5586630953226452\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  0.39997209858632793\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  0.46229442906226015\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  1.804223831315938\n",
      "length:  34\n",
      "distance:  2\n",
      "score randomness:  1.5805177656115839\n",
      "length:  34\n",
      "distance:  4\n",
      "score randomness:  0.15969574935496741\n",
      "length:  34\n",
      "distance:  2\n",
      "score randomness:  0.40181483621558917\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  2.150393294926973\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  1.718771918180363\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  0.1537588768442699\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  1.0969097934321212\n",
      "length:  34\n",
      "distance:  4\n",
      "score randomness:  1.4290071783056204\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  0.29112850995341505\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  0.4486350372871501\n",
      "length:  34\n",
      "distance:  1\n",
      "score randomness:  0.6597630341039031\n",
      "length:  34\n",
      "distance:  1\n",
      "score randomness:  1.5065417158337735\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  1.1261971515639604\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  1.1317785857723037\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  1.6369206524102626\n",
      "length:  34\n",
      "distance:  4\n",
      "score randomness:  1.13829823171424\n",
      "length:  34\n",
      "distance:  2\n",
      "score randomness:  0.564724688329461\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  1.1499527717457647\n",
      "length:  34\n",
      "distance:  1\n",
      "score randomness:  1.2518793759626365\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  0.17802588632573996\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  0.8577244088697116\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  1.1032013218294618\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  1.5771034560905721\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  0.23228219206005243\n",
      "length:  34\n",
      "distance:  4\n",
      "score randomness:  0.1681842473884701\n",
      "length:  34\n",
      "distance:  5\n",
      "score randomness:  0.3076301833769002\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  0.28792453550733565\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  0.22733604103751925\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  0.2842140363518165\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  0.5407682879250092\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  0.2709744889229013\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  0.23596090555617247\n",
      "length:  34\n",
      "distance:  8\n",
      "score randomness:  1.1409649304158442\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  1.2690980217271104\n",
      "length:  34\n",
      "distance:  8\n",
      "score randomness:  0.6473583896851204\n",
      "length:  34\n",
      "distance:  6\n",
      "score randomness:  1.6831666079444387\n",
      "length:  34\n",
      "distance:  7\n",
      "score randomness:  1.0648137144486451\n",
      "length:  34\n",
      "distance:  2\n",
      "score randomness:  0.345646117754419\n",
      "length:  34\n",
      "distance:  1\n",
      "score randomness:  0.540333508259996\n",
      "length:  34\n",
      "distance:  3\n",
      "score randomness:  1.8647287518084141\n",
      "length:  35\n",
      "distance:  0\n",
      "score randomness:  2.869019902947727\n",
      ".  .  .  .  .  .  .  .  .  .  \n",
      "=  o  =  =  .  .  .  .  .  .  \n",
      "=  =  .  =  =  .  .  .  .  .  \n",
      "=  =  =  =  =  .  .  .  .  .  \n",
      "=  =  .  .  .  .  .  .  .  .  \n",
      ".  =  =  .  .  .  .  .  .  .  \n",
      ".  .  =  =  .  .  -  .  .  @\n",
      ".  .  =  =  .  =  =  .  .  .  \n",
      ".  .  =  =  =  =  .  .  .  .  \n",
      ".  .  =  =  .  .  .  .  .  .  \n",
      "\n",
      "failure\n",
      "{'Score': 32, '# Moves': 416, 'Termination': 'eaten itself'}\n",
      "collected! Starting again ...\n"
     ]
    }
   ],
   "source": [
    "run_experiment(stat, [a1], snake_g, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41675d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "file_name = 'experiment_snake_h3_' + current_datetime_str + '.csv'\n",
    "df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9381ef6d",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73bcc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_gm = ChessGame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "587387c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST: minimax alpha_beta h1 vs minimax h1, DEPTH 2\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=1)\n",
    "a2 = Agent(Algorithms.minimax_search, Heuristics.chess_h1, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31199997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r . . . . . . r\n",
      ". . . . k Q . p\n",
      ". q . . P . . .\n",
      ". . . . . . . .\n",
      "p . . R . . P .\n",
      ". . . . . P . .\n",
      ". . . . . K . .\n",
      ". . B . . . N .\n",
      "game ended!\n"
     ]
    }
   ],
   "source": [
    "for j in range(0,5):\n",
    "\n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h1_alpha_beta_vs_minimax_depth_1_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d10576a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r n . q . r . .\n",
      ". . . . . . b p\n",
      ". p . N b . k n\n",
      ". . p p p p . .\n",
      "p P P P . P p K\n",
      "P . . . . . P .\n",
      ". . R . P N B P\n",
      ". . B . Q . . R\n",
      "game ended!\n"
     ]
    }
   ],
   "source": [
    "# TEST: minimax alpha_beta h1 vs minimax h1, DEPTH 2\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=2)\n",
    "a2 = Agent(Algorithms.minimax_search, Heuristics.chess_h1, level=2)\n",
    "\n",
    "for j in range(0,5):\n",
    "    \n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h1_alpha_beta_vs_minimax_depth_2_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35e633df",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m      7\u001b[0m     i \u001b[38;5;241m=\u001b[39m Infrastructure([a1, a2], chess_gm, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     current_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     10\u001b[0m     current_datetime_str \u001b[38;5;241m=\u001b[39m current_datetime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 118\u001b[0m, in \u001b[0;36mInfrastructure.main\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents)):\n\u001b[0;32m    114\u001b[0m     \n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# l'agente i-esimo sceglie la propria azione\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     starting_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 118\u001b[0m     current_agent_action, explored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     finishing_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    122\u001b[0m     delta_t \u001b[38;5;241m=\u001b[39m finishing_time \u001b[38;5;241m-\u001b[39m starting_time\n",
      "Cell \u001b[1;32mIn[12], line 47\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel must be an integer >= 0!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# gli algoritmi minimax e alpha-beta pruning restituiscono una tupla formata da:\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# - mossa selezionata\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# - valutazione dello stato risultante dall'applicazione di tale mossa\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheuristic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 193\u001b[0m, in \u001b[0;36mAlgorithms.minimax_search\u001b[1;34m(s, l, h)\u001b[0m\n\u001b[0;32m    190\u001b[0m explored \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[1;32m--> 193\u001b[0m     valuation, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmin_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m#explored += newly_explored\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     valuated_actions\u001b[38;5;241m.\u001b[39mappend((action, valuation))\n",
      "Cell \u001b[1;32mIn[11], line 164\u001b[0m, in \u001b[0;36mAlgorithms.minimax_search.<locals>.min_value\u001b[1;34m(s, l, h, explored)\u001b[0m\n\u001b[0;32m    162\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[1;32m--> 164\u001b[0m     res, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmax_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(v, res)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m#explored += newly_explored\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 183\u001b[0m, in \u001b[0;36mAlgorithms.minimax_search.<locals>.max_value\u001b[1;34m(s, l, h, explored)\u001b[0m\n\u001b[0;32m    181\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[1;32m--> 183\u001b[0m     res, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmin_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(v, res)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v, explored\n",
      "Cell \u001b[1;32mIn[11], line 164\u001b[0m, in \u001b[0;36mAlgorithms.minimax_search.<locals>.min_value\u001b[1;34m(s, l, h, explored)\u001b[0m\n\u001b[0;32m    162\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[1;32m--> 164\u001b[0m     res, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmax_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(v, res)\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m#explored += newly_explored\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 178\u001b[0m, in \u001b[0;36mAlgorithms.minimax_search.<locals>.max_value\u001b[1;34m(s, l, h, explored)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h(s), explored\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m, explored\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# other case\u001b[39;00m\n\u001b[0;32m    181\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 149\u001b[0m, in \u001b[0;36mHeuristics.chess_h1\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    146\u001b[0m black_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m64\u001b[39m):\n\u001b[1;32m--> 149\u001b[0m     piece \u001b[38;5;241m=\u001b[39m \u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpiece_at\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_square\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSQUARE_NAMES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# se in uno square non si trova alcuna pedina, viene ritornato None\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m piece \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TEST: minimax alpha_beta h1 vs minimax h1, DEPTH 3\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=3)\n",
    "a2 = Agent(Algorithms.minimax_search, Heuristics.chess_h1, level=3)\n",
    "\n",
    "for j in range(0,5):\n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h1_alpha_beta_vs_minimax_depth_3_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c30ba83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . . k r\n",
      ". . . . . . b p\n",
      ". . p . Q . p .\n",
      ". . . . . . . q\n",
      ". . . . P . . .\n",
      "B . P . . . . N\n",
      ". . . P . P P P\n",
      "R . . . . . K .\n",
      "game ended!\n"
     ]
    }
   ],
   "source": [
    "# TEST: minimax alpha_beta h1 vs alpha_beta h1, player 1 depth = 3; player 2 depth = 1\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=3)\n",
    "a2 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=1)\n",
    "\n",
    "for j in range(0, 5):\n",
    "\n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    \n",
    "    # saving the dataset\n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h1_alpha_beta_vs_alpha_beta_depth_3_1_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f93be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . k r b n r\n",
      "p . . Q p . p .\n",
      ". . . . . . . .\n",
      ". B . . . p . p\n",
      ". N . P P P . .\n",
      ". . . K . . . P\n",
      "P P P B . . . P\n",
      "R . . . . . N R\n",
      "game ended!\n"
     ]
    }
   ],
   "source": [
    "# TEST: minimax alpha_beta h1 vs alpha_beta h1, player 1 depth = 3; player 2 depth = 2\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=3)\n",
    "a2 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=2)\n",
    "\n",
    "for j in range(0,5):\n",
    "\n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    \n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h1_alpha_beta_vs_alpha_beta_depth_3_2_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf83b808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r n b . k b n r\n",
      ". p p p . p p p\n",
      ". . . . . . . .\n",
      "p . . . p . . .\n",
      ". . . . . . P q\n",
      ". P . . . P . .\n",
      "P . P P P . . P\n",
      "R N B Q K B N R\n",
      "game ended!\n"
     ]
    }
   ],
   "source": [
    "# TEST: minimax alpha_beta h1 vs alpha_beta h1, player 1 depth = player 2 depth = 3\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=3)\n",
    "a2 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=3)\n",
    "\n",
    "for j in range(0,5):\n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    \n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h1_alpha_beta_vs_alpha_beta_depth_3_3_1_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d8587e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m i \u001b[38;5;241m=\u001b[39m Infrastructure([a1, a2], chess_gm, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m current_datetime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     12\u001b[0m current_datetime_str \u001b[38;5;241m=\u001b[39m current_datetime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[59], line 118\u001b[0m, in \u001b[0;36mInfrastructure.main\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents)):\n\u001b[0;32m    114\u001b[0m     \n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# l'agente i-esimo sceglie la propria azione\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     starting_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 118\u001b[0m     current_agent_action, explored \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magents\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m     finishing_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    122\u001b[0m     delta_t \u001b[38;5;241m=\u001b[39m finishing_time \u001b[38;5;241m-\u001b[39m starting_time\n",
      "Cell \u001b[1;32mIn[58], line 47\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel must be an integer >= 0!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# gli algoritmi minimax e alpha-beta pruning restituiscono una tupla formata da:\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# - mossa selezionata\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# - valutazione dello stato risultante dall'applicazione di tale mossa\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malgorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheuristic\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[57], line 278\u001b[0m, in \u001b[0;36mAlgorithms.alpha_beta_search\u001b[1;34m(s, l, h)\u001b[0m\n\u001b[0;32m    272\u001b[0m explored \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[0;32m    275\u001b[0m     \n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# initial alpha value = - infinity\u001b[39;00m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;66;03m# initial beta value = + infinity\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m     valuation, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmin_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m     valuated_actions\u001b[38;5;241m.\u001b[39mappend((action, valuation))\n\u001b[0;32m    281\u001b[0m sorted_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(valuated_actions, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[57], line 239\u001b[0m, in \u001b[0;36mAlgorithms.alpha_beta_search.<locals>.min_value\u001b[1;34m(s, alpha, beta, l, h, explored)\u001b[0m\n\u001b[0;32m    237\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[1;32m--> 239\u001b[0m     res, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmax_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(v, res)\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m alpha:\n",
      "Cell \u001b[1;32mIn[57], line 261\u001b[0m, in \u001b[0;36mAlgorithms.alpha_beta_search.<locals>.max_value\u001b[1;34m(s, alpha, beta, l, h, explored)\u001b[0m\n\u001b[0;32m    259\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[1;32m--> 261\u001b[0m     res, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmin_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(v, res)\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m beta:\n",
      "Cell \u001b[1;32mIn[57], line 239\u001b[0m, in \u001b[0;36mAlgorithms.alpha_beta_search.<locals>.min_value\u001b[1;34m(s, alpha, beta, l, h, explored)\u001b[0m\n\u001b[0;32m    237\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39mactions():\n\u001b[1;32m--> 239\u001b[0m     res, explored \u001b[38;5;241m=\u001b[39m \u001b[43mmax_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplored\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(v, res)\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m alpha:\n",
      "Cell \u001b[1;32mIn[57], line 253\u001b[0m, in \u001b[0;36mAlgorithms.alpha_beta_search.<locals>.max_value\u001b[1;34m(s, alpha, beta, l, h, explored)\u001b[0m\n\u001b[0;32m    250\u001b[0m explored \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;66;03m# base cases\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h(s), explored\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[27], line 35\u001b[0m, in \u001b[0;36mState.is_final\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_final\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_final_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 76\u001b[0m, in \u001b[0;36mChessGame.is_final_config\u001b[1;34m(self, configuration)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_final_config\u001b[39m(\u001b[38;5;28mself\u001b[39m, configuration:chess\u001b[38;5;241m.\u001b[39mBoard):\n\u001b[0;32m     74\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"check whether the configuration is final or not\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m        se configuration.board is None, then the game is not concluded \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI1\\lib\\site-packages\\chess\\__init__.py:1962\u001b[0m, in \u001b[0;36mBoard.outcome\u001b[1;34m(self, claim_draw)\u001b[0m\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_insufficient_material():\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Outcome(Termination\u001b[38;5;241m.\u001b[39mINSUFFICIENT_MATERIAL, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_legal_moves\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Outcome(Termination\u001b[38;5;241m.\u001b[39mSTALEMATE, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;66;03m# Automatic draws.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI1\\lib\\site-packages\\chess\\__init__.py:3589\u001b[0m, in \u001b[0;36mBoard.generate_legal_moves\u001b[1;34m(self, from_mask, to_mask)\u001b[0m\n\u001b[0;32m   3587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m king_mask:\n\u001b[0;32m   3588\u001b[0m     king \u001b[38;5;241m=\u001b[39m msb(king_mask)\n\u001b[1;32m-> 3589\u001b[0m     blockers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slider_blockers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3590\u001b[0m     checkers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattackers_mask(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturn, king)\n\u001b[0;32m   3591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m checkers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\AI1\\lib\\site-packages\\chess\\__init__.py:3531\u001b[0m, in \u001b[0;36mBoard._slider_blockers\u001b[1;34m(self, king)\u001b[0m\n\u001b[0;32m   3526\u001b[0m rooks_and_queens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrooks \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueens\n\u001b[0;32m   3527\u001b[0m bishops_and_queens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbishops \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueens\n\u001b[0;32m   3529\u001b[0m snipers \u001b[38;5;241m=\u001b[39m ((BB_RANK_ATTACKS[king][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m&\u001b[39m rooks_and_queens) \u001b[38;5;241m|\u001b[39m\n\u001b[0;32m   3530\u001b[0m            (BB_FILE_ATTACKS[king][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m&\u001b[39m rooks_and_queens) \u001b[38;5;241m|\u001b[39m\n\u001b[1;32m-> 3531\u001b[0m            (\u001b[43mBB_DIAG_ATTACKS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mking\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m&\u001b[39m bishops_and_queens))\n\u001b[0;32m   3533\u001b[0m blockers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3535\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sniper \u001b[38;5;129;01min\u001b[39;00m scan_reversed(snipers \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moccupied_co[\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mturn]):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TEST: minimax alpha_beta h2 vs alpha_beta h1, player 1 depth = player 2 depth = 3\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h2, level=3)\n",
    "a2 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h1, level=3)\n",
    "\n",
    "for j in range(0,10):\n",
    "    print(f'game {j+1}')\n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    \n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h2_alpha_beta_vs_h1_alpha_beta_depth_3_3_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ded2ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". . . . . r k .\n",
      ". p . . . . . .\n",
      "p . . n . . . .\n",
      ". . . r . . . p\n",
      ". P . . b K p .\n",
      "N . . . P . P .\n",
      "P . . . . . . P\n",
      ". . . . R B R .\n",
      "game ended!\n"
     ]
    }
   ],
   "source": [
    "# TEST: minimax alpha_beta h2 vs alpha_beta h2, player 1 depth = player 2 depth = 3\n",
    "\n",
    "a1 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h2, level=3)\n",
    "a2 = Agent(Algorithms.alpha_beta_search, Heuristics.chess_h2, level=3)\n",
    "\n",
    "for j in range(0,6):\n",
    "    print(f'game {j+1}')\n",
    "    i = Infrastructure([a1, a2], chess_gm, None)\n",
    "    df = i.main()\n",
    "    \n",
    "    current_datetime = datetime.datetime.now()\n",
    "    current_datetime_str = current_datetime.strftime('%Y_%m_%d %H_%M_%S')\n",
    "    file_name = 'h2_alpha_beta_vs_h2_alpha_beta_depth_3_3_'\n",
    "    file_name += current_datetime_str + '.csv'\n",
    "    df.to_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e5298b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
